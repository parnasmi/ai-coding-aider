from typing import Optional
from pydantic import BaseModel

class EvaluationResult(BaseModel):
    success: bool
    feedback: Optional[str]

class DirectorConfig(BaseModel):
    prompt: str
    coder_model: str
    evaluator_model: Literal["gpt-4o", "gpt-4o-mini", "o1-mini", "o1-preview"]
    max_iterations: int
    execution_command: str
    context_editable: List[str]
    context_read_only: List[str]
    evaluator: Literal["default"]

class Director:
    def __init__(self, config_path: str):
        """Initialize with a validated configuration and OpenAI client."""
        self.config = self.validate_config(Path(config_path))
        self.llm_client = OpenAI()
    
    @staticmethod
    # def validate_config(config_path: Path) -> DirectorConfig:
    # def parse_llm_json_response(self, response: str) -> str:
    # def file_log(self, message: str, print_message: bool = True):

    # ----------- Key Director Methods -----------
    

    def create_new_ai_coding_prompt(
    self,
    iteration: int,
    base_input_prompt: str,
    execution_output: str,
    evaluation: EvaluationResult,
    ) -> str:
        if iteration == 0:
            return base_input_prompt
        else:
            return f"""
            # Generate the next iteration of code to achieve the user's desired result based on their original instructions and the feedback from the previous attempt.
            > Generate a new prompt in the same style as the original instructions for the next iteration of code.

            ## This is your {iteration}th attempt to generate the code.
            > You have {self.config.max_iterations - iteration} attempts remaining.

            ## Here's the user's original instructions for generating the code:
            {base_input_prompt}

            ## Here's the output of your previous attempt:
            {execution_output}

            ## Here's feedback on your previous attempt:
            {evaluation.feedback}"""
    def ai_code(self, prompt: str):
        model = Model(self.config.coder_model)
        coder = Coder.create(
            main_model=model,
            io=InputOutput(yes=True),
            fnames=self.config.context_editable,
            read_only_fnames=self.config.context_read_only,
            auto_commits=False,
            suggest_shell_commands=False,
        )
        coder.run(prompt)
    
    def execute(self) -> str:
        """Execute the tests and return the output as a string."""
        result = subprocess.run(
            self.config.execution_command.split(),
            capture_output=True,
            text=True,
        )

        self.file_log(
            f"Execution output: \n{result.stdout + result.stderr}",
            print_message=False,
        )

        return result.stdout + result.stderr
    def evaluate(self, execution_output: str) -> EvaluationResult:
        if self.config.evaluator != "default":
            raise ValueError(
                f"Custom evaluator {self.config.evaluator} not implemented"
            )

        map_editable_fname_to_files = {
            Path(fname).name: Path(fname).read_text()
            for fname in self.config.context_editable
        }

        map_read_only_fname_to_files = {
            Path(fname).name: Path(fname).read_text()
            for fname in self.config.context_read_only
        }

        evaluation_prompt = f"""Evaluate this execution output and determine if it was successful based on the execution command, the user's desired result,

        ## Checklist:
        - Is the execution output reporting success or failure?
        - Did we miss any tasks? Review the User's Desired Result to see if we have satisfied all tasks.
        - Did we satisfy the user's desired result?
        - Ignore warnings

        ## User's Desired Result:
        {self.config.prompt}

        ## Editable Files:
        {map_editable_fname_to_files}

        ## Read-Only Files:
        {map_read_only_fname_to_files}

        ## Execution Command:
        {self.config.execution_command}

        ## Execution Output:
        {execution_output}

        ## Response Format:
        > Be 100% sure to output JSON.parse compatible JSON.
        > That means no new lines.

        Return a structured JSON response with the following structure: {{
            success: bool - true if the execution output generated by the execution command matches the Users Desired Result
            feedback: str | None - if unsuccessful, provide detailed feedback explaining what failed and how to fix it, or None if successful
        }}"""

        self.file_log(
            f"Evaluation prompt: ({self.config.evaluator_model}):\n{evaluation_prompt}",
            print_message=False,
        )

        try:
            completion = self.llm_client.chat.completions.create(
                model=self.config.evaluator_model,
                messages=[
                    {
                        "role": "user",
                        "content": evaluation_prompt,
                    },
                ],
            )

            self.file_log(
                f"Evaluation response: ({self.config.evaluator_model}):\n{completion.choices[0].message.content}",
                print_message=False,
            )

            evaluation = EvaluationResult.model_validate_json(
                self.parse_llm_json_response(completion.choices[0].message.content)
            )

            return evaluation
        except Exception as e:
            self.file_log(
                f"Error evaluating execution output for '{self.config.evaluator_model}'. Error: {e}. Falling back to gpt-4o & structured output."
            )

            ## Fallback
            completion = self.llm_client.beta.chat.completions.parse(
                model="gpt-4o",
                messages=[
                    {
                        "role": "user",
                        "content": evaluation_prompt,
                    },
                ],
                response_format=EvaluationResult,
            )

            message = completion.choices[0].message
            if message.parsed:
                return message.parsed
            else:
                raise ValueError("Failed to parse the response")
            
    def direct(self):
        evaluation = EvaluationResult(success=False, feedback=None)
        execution_output = ""
        success = False

        for i in range(self.config.max_iterations):
            self.file_log(f"\nIteration {i+1}/{self.config.max_iterations}")

            self.file_log("üß† Creating new prompt...")
            new_prompt = self.create_new_ai_coding_prompt(
                i, self.config.prompt, execution_output, evaluation
            )

            self.file_log("ü§ñ Generating AI code...")
            self.ai_code(new_prompt)

            self.file_log(f"üíª Executing code... '{self.config.execution_command}'")
            execution_output = self.execute()

            self.file_log(
                f"üîç Evaluating results... '{self.config.evaluator_model}' + '{self.config.evaluator}'"
            )
            evaluation = self.evaluate(execution_output)

            self.file_log(
                f"üîç Evaluation result: {'‚úÖ Success' if evaluation.success else '‚ùå Failed'}"
            )

            if evaluation.feedback:
                self.file_log(f"üí¨ Feedback: \n{evaluation.feedback}")
            
            if evaluation.success:
                success = True
                self.file_log(
                    f"\nüéâ Success achieved after {i+1} iterations! Breaking out of iteration loop."
                )
                break
            else:
                self.file_log(
                    f"\nüîÑ Continuing with next iteration... Have {self.config.max_iterations - i - 1} attempts remaining."
                )
        if not success:
            self.file_log(
                "\nüö´ Failed to achieve success within the maximum number of iterations."
            )

        self.file_log("\nDone.")

# if __name__ == "__main__":
#     director = Director(config_path="path/to/config.yaml")
#     director.direct()